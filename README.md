# aws-data-pipelines

This public repository documents a focused six-month learning journey to build production-grade, end-to-end data pipelines using AWS core services and modern engineering practices.

## Objective

To gain hands-on expertise in designing scalable, resilient data workflows on AWS. Focus areas include:

- Ingestion pipelines using AWS Glue, Kinesis, Lambda  
- Storage and transformation with S3, Glue, and Athena  
- Orchestration with Step Functions and MWAA (Managed Workflows for Apache Airflow)  
- Monitoring and logging via CloudWatch and CloudTrail  
- Infrastructure as code and CI/CD using Terraform and AWS CodePipeline


## Learning Roadmap
This six-month roadmap is structured into focused phases that reflect real-world data engineering workflows.

| Phase                                      | Duration | Focus                                                       |
| ------------------------------------------ | -------- | ----------------------------------------------------------- |
| **Phase 1: Foundations**                   | 2 weeks  | IAM, S3, CLI, Boto3 basics                                  |
| **Phase 2: Ingestion**                     | 3 weeks  | Kinesis streams, Glue Crawlers, Lambda, DMS                 |
| **Phase 3: ETL & Storage**                 | 4 weeks  | Glue (PySpark), Delta formats, schema evolution             |
| **Phase 4: Querying & Reporting**          | 2 weeks  | Athena for ad-hoc queries, QuickSight for dashboards        |
| **Phase 5: Orchestration & Monitoring**    | 4 weeks  | Step Functions, MWAA (Airflow), observability with logs     |
| **Phase 6: CI/CD & Infrastructure**        | 4 weeks  | Terraform IaC, CodePipeline, multi-env deployments          |
| **Phase 7: Capstone & Resume Integration** | 2 weeks  | End-to-end project and publishing to repo                   |

This repository is actively updated. All projects are designed with code-first principles, version control, and reproducibility.
