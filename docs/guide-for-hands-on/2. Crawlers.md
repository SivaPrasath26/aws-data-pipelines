A crawler in AWS Glue scans your S3 files, detects schema, and creates tables in the Glue Data Catalog.

It's how Glue knows what data you have and how it's structured.

Steps:
## 1 Go to AWS Console → Glue → Crawlers
![alt text](image/image-13.png)

## 2 Click Create Crawler
![alt text]image/image-14.png)

## 3 Choose data store: S3, and select your bucket path.
![alt text](image/image-15.png)

## 4 Set IAM role: Use existing or create a new one with Glue access.

* Choose:
"Create new IAM role" → then select "Glue" as the service that can assume the role.

* During setup:

Name it something like AWSGlueServiceRole-MyProject

![alt text](image/image-16.png)
* Ensure it has permission to access your S3 bucket (you can manually add AmazonS3ReadOnlyAccess or custom S3 access later if needed)

* If permission is denied to create iam role, go to your root account and select the iam user and add the below permission manually 
<img width="1830" height="727" alt="image" src="https://github.com/user-attachments/assets/e41a4e85-63da-459d-bd0a-65f6efeea053" />

<img width="1497" height="767" alt="image" src="https://github.com/user-attachments/assets/2b311dc1-6ce7-4ff0-b784-57eb74bfe963" />

* Let Glue use this role to read your uploaded CSV files and write metadata to the Glue Data Catalog.

## 5 Set output: Choose a database or create one.
<img width="1440" height="732" alt="image" src="https://github.com/user-attachments/assets/472485d9-a1b2-4fc2-9b1c-69a41d47e4b4" />
<img width="1068" height="730" alt="image" src="https://github.com/user-attachments/assets/3407bffe-d9fc-48ec-933c-51b0f89224e3" />

## 6 Check all and create crawler
<img width="1460" height="817" alt="image" src="https://github.com/user-attachments/assets/2ca184c4-2fd1-4e65-951e-8f74e3598965" />

<img width="1074" height="723" alt="image" src="https://github.com/user-attachments/assets/c79a8f6a-3cf0-4597-b6fb-fed7826f8ed7" />

## 7 Run the crawler

<img width="1020" height="756" alt="image" src="https://github.com/user-attachments/assets/08f2a494-648c-4bb0-84c0-87f8855ffe16" />

* Monitor the run
<img width="1020" height="475" alt="image" src="https://github.com/user-attachments/assets/fdd15da1-aed8-46ba-9dd7-a28d7f1a151a" />

## 8 Once done refresh db and check if the files got ingested
<img width="1453" height="733" alt="image" src="https://github.com/user-attachments/assets/7f18cf87-b5d2-4d85-ba09-594ff57b9261" />
